{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47d8d3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaffba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/bert_uncased_L-2_H-128_A-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54cae9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de89b58f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (C:\\Users\\faizan\\.cache\\huggingface\\datasets\\glue\\cola\\1.0.0\\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "cola_dataset = load_dataset('glue', 'cola')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b209a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 8551\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1043\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx'],\n",
       "        num_rows: 1063\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d93d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = cola_dataset['train']\n",
    "val_set = cola_dataset['validation']\n",
    "test_set = cola_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9b082d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples in the training set: 8551\n",
      "total examples in the validation set: 1043\n",
      "total examples in the test set: 1063\n"
     ]
    }
   ],
   "source": [
    "print(f'total examples in the training set: {len(train_set)}')\n",
    "print(f'total examples in the validation set: {len(val_set)}')\n",
    "print(f'total examples in the test set: {len(test_set)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e4cf05f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training sample: \n",
      "\n",
      "{'idx': 905, 'label': 1, 'sentence': \"Joe's neuroses bother his patrons, and Sally's neuroses do too.\"}\n",
      "\n",
      "validation sample: \n",
      "\n",
      "{'idx': 905, 'label': 1, 'sentence': 'Lou hoped the umbrella was broken.'}\n",
      "\n",
      "test sample: \n",
      "\n",
      "{'idx': 905, 'label': -1, 'sentence': 'Dale gave Brooke a hard time.'}\n"
     ]
    }
   ],
   "source": [
    "# show a sample example from each sets\n",
    "# training set\n",
    "import random\n",
    "\n",
    "rand_index = random.randint(0, len(test_set))\n",
    "train_sample = train_set[rand_index]\n",
    "val_sample = val_set[rand_index]\n",
    "test_sample = test_set[rand_index]\n",
    "print('training sample: \\n')\n",
    "print(train_sample)\n",
    "print()\n",
    "\n",
    "print('validation sample: \\n')\n",
    "print(val_sample)\n",
    "print()\n",
    "\n",
    "print('test sample: \\n')\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c0cb6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['unacceptable', 'acceptable'], names_file=None, id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b351bd34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='google/bert_uncased_L-2_H-128_A-2', vocab_size=30522, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b61eb3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training sample sentence: Joe's neuroses bother his patrons, and Sally's neuroses do too.\n"
     ]
    }
   ],
   "source": [
    "print(f'training sample sentence: {train_sample[\"sentence\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d946a3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing the sentence: {'input_ids': [101, 3533, 1005, 1055, 11265, 10976, 8583, 8572, 2010, 13497, 1010, 1998, 8836, 1005, 1055, 11265, 10976, 8583, 2079, 2205, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(f'tokenizing the sentence: {tokenizer(train_sample[\"sentence\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "442859ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding the tokenizer output: [CLS] joe's neuroses bother his patrons, and sally's neuroses do too. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Decoding the tokenizer output: {tokenizer.decode(tokenizer(train_sample['sentence'])['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71946402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(samples):\n",
    "    return tokenizer(\n",
    "        samples['sentence'],\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        max_length = 512\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acff494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function encode at 0x00000219C6DDD4C0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|██████████| 9/9 [00:03<00:00,  2.66ba/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_set.map(encode, batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c175391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type = 'torch', columns= [\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84695316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence', 'token_type_ids'],\n",
       "    num_rows: 8551\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9184084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loaders \n",
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size= 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cac8f298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'input_ids': tensor([[  101,  2256,  2814,  ...,     0,     0,     0],\n",
       "         [  101,  2028,  2062,  ...,     0,     0,     0],\n",
       "         [  101,  2028,  2062,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [  101,  5965, 12808,  ...,     0,     0,     0],\n",
       "         [  101,  2198, 10948,  ...,     0,     0,     0],\n",
       "         [  101,  3021, 24471,  ...,     0,     0,     0]]),\n",
       " 'label': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "         1, 0, 0, 1, 1, 1, 1, 1])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3b8a04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n",
      "torch.Size([32, 512]) torch.Size([32, 512]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(dataloader):\n",
    "    print(batch['input_ids'].shape, batch['attention_mask'].shape, batch['label'].shape)\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2135e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3bf9387",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 17.7M/17.7M [00:34<00:00, 509kB/s] \n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b4a4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df5a74fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(model.config.hidden_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5aef71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "929c8ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7750f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
