[2023-08-07 16:06:08,589][__main__][INFO] - model:
  name: google/bert_uncased_L-2_H-128_A-2
  tokenizer: google/bert_uncased_L-2_H-128_A-2
preprocess:
  batch: 64
  max_length: 128
training:
  max_epochs: 2
  log_every_n_steps: 10
  deterministic: true
  limit_train_batches: 0.25
  limit_val_batches: 0.25

[2023-08-07 16:06:08,590][__main__][INFO] - Using model: google/bert_uncased_L-2_H-128_A-2
[2023-08-07 16:06:08,590][__main__][INFO] - using the tokenizer: google/bert_uncased_L-2_H-128_A-2
[2023-08-07 16:06:17,476][datasets.builder][WARNING] - Reusing dataset glue (C:\Users\faizan\.cache\huggingface\datasets\glue\cola\1.0.0\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
[2023-08-07 16:06:19,516][datasets.fingerprint][WARNING] - Parameter 'function'=<bound method Dataset.tokenize of <dataset.Dataset object at 0x0000027C53631D30>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
[2023-08-07 16:12:37,932][__main__][INFO] - model:
  name: google/bert_uncased_L-2_H-128_A-2
  tokenizer: google/bert_uncased_L-2_H-128_A-2
preprocess:
  batch: 64
  max_length: 128
training:
  max_epochs: 2
  log_every_n_steps: 10
  deterministic: true
  limit_train_batches: 0.25
  limit_val_batches: 0.25

[2023-08-07 16:12:37,935][__main__][INFO] - Using model: google/bert_uncased_L-2_H-128_A-2
[2023-08-07 16:12:37,936][__main__][INFO] - using the tokenizer: google/bert_uncased_L-2_H-128_A-2
[2023-08-07 16:12:45,985][datasets.builder][WARNING] - Reusing dataset glue (C:\Users\faizan\.cache\huggingface\datasets\glue\cola\1.0.0\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
[2023-08-07 16:12:47,968][datasets.fingerprint][WARNING] - Parameter 'function'=<bound method Dataset.tokenize of <dataset.Dataset object at 0x0000021573681DC0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
[2023-08-08 19:17:43,284][__main__][INFO] - model:
  name: google/bert_uncased_L-2_H-128_A-2
  tokenizer: google/bert_uncased_L-2_H-128_A-2
preprocess:
  batch: 64
  max_length: 128
training:
  max_epochs: 2
  log_every_n_steps: 10
  deterministic: true
  limit_train_batches: 0.25
  limit_val_batches: 0.25

[2023-08-08 19:17:43,285][__main__][INFO] - Using model: google/bert_uncased_L-2_H-128_A-2
[2023-08-08 19:17:43,286][__main__][INFO] - using the tokenizer: google/bert_uncased_L-2_H-128_A-2
[2023-08-08 19:17:51,182][datasets.builder][WARNING] - Reusing dataset glue (C:\Users\faizan\.cache\huggingface\datasets\glue\cola\1.0.0\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
[2023-08-08 19:17:53,517][datasets.fingerprint][WARNING] - Parameter 'function'=<bound method Dataset.tokenize of <dataset.Dataset object at 0x000001E657E42D60>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
[2023-08-08 19:20:05,849][__main__][INFO] - model:
  name: google/bert_uncased_L-2_H-128_A-2
  tokenizer: google/bert_uncased_L-2_H-128_A-2
preprocess:
  batch: 64
  max_length: 128
training:
  max_epochs: 1
  log_every_n_steps: 10
  deterministic: true
  limit_train_batches: 0.25
  limit_val_batches: 0.25

[2023-08-08 19:20:05,851][__main__][INFO] - Using model: google/bert_uncased_L-2_H-128_A-2
[2023-08-08 19:20:05,853][__main__][INFO] - using the tokenizer: google/bert_uncased_L-2_H-128_A-2
[2023-08-08 19:20:15,495][datasets.builder][WARNING] - Reusing dataset glue (C:\Users\faizan\.cache\huggingface\datasets\glue\cola\1.0.0\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
[2023-08-08 19:20:18,781][datasets.fingerprint][WARNING] - Parameter 'function'=<bound method Dataset.tokenize of <dataset.Dataset object at 0x00000214E8C32CA0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
[2023-08-08 19:44:30,484][__main__][INFO] - model:
  name: google/bert_uncased_L-2_H-128_A-2
  tokenizer: google/bert_uncased_L-2_H-128_A-2
preprocess:
  batch: 64
  max_length: 128
training:
  max_epochs: 1
  log_every_n_steps: 10
  deterministic: true
  limit_train_batches: 0.25
  limit_val_batches: 0.25

[2023-08-08 19:44:30,484][__main__][INFO] - Using model: google/bert_uncased_L-2_H-128_A-2
[2023-08-08 19:44:30,485][__main__][INFO] - using the tokenizer: google/bert_uncased_L-2_H-128_A-2
[2023-08-08 19:44:38,993][datasets.builder][WARNING] - Reusing dataset glue (C:\Users\faizan\.cache\huggingface\datasets\glue\cola\1.0.0\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
[2023-08-08 19:44:41,392][datasets.fingerprint][WARNING] - Parameter 'function'=<bound method Dataset.tokenize of <dataset.Dataset object at 0x0000021D83F22DC0>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
[2023-08-08 19:53:59,310][__main__][INFO] - model:
  name: google/bert_uncased_L-2_H-128_A-2
  tokenizer: google/bert_uncased_L-2_H-128_A-2
preprocess:
  batch: 64
  max_length: 128
training:
  max_epochs: 1
  log_every_n_steps: 10
  deterministic: true
  limit_train_batches: 0.25
  limit_val_batches: 0.25

[2023-08-08 19:53:59,310][__main__][INFO] - Using model: google/bert_uncased_L-2_H-128_A-2
[2023-08-08 19:53:59,311][__main__][INFO] - using the tokenizer: google/bert_uncased_L-2_H-128_A-2
[2023-08-08 19:54:07,499][datasets.builder][WARNING] - Reusing dataset glue (C:\Users\faizan\.cache\huggingface\datasets\glue\cola\1.0.0\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
[2023-08-08 19:54:09,157][datasets.fingerprint][WARNING] - Parameter 'function'=<bound method Dataset.tokenize of <dataset.Dataset object at 0x00000247BC302D30>> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.
[2023-08-09 11:33:05,396][__main__][INFO] - model:
  name: google/bert_uncased_L-2_H-128_A-2
  tokenizer: google/bert_uncased_L-2_H-128_A-2
preprocess:
  batch: 64
  max_length: 128
training:
  max_epochs: 1
  log_every_n_steps: 10
  deterministic: true
  limit_train_batches: 0.25
  limit_val_batches: 0.25

[2023-08-09 11:33:05,397][__main__][INFO] - Using model: google/bert_uncased_L-2_H-128_A-2
[2023-08-09 11:33:05,397][__main__][INFO] - using the tokenizer: google/bert_uncased_L-2_H-128_A-2
[2023-08-09 11:33:15,721][datasets.builder][WARNING] - Reusing dataset glue (C:\Users\faizan\.cache\huggingface\datasets\glue\cola\1.0.0\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
[2023-08-09 11:37:27,193][__main__][INFO] - model:
  name: google/bert_uncased_L-2_H-128_A-2
  tokenizer: google/bert_uncased_L-2_H-128_A-2
preprocess:
  batch: 64
  max_length: 128
training:
  max_epochs: 1
  log_every_n_steps: 10
  deterministic: true
  limit_train_batches: 0.25
  limit_val_batches: 0.25

[2023-08-09 11:37:27,193][__main__][INFO] - Using model: google/bert_uncased_L-2_H-128_A-2
[2023-08-09 11:37:27,194][__main__][INFO] - using the tokenizer: google/bert_uncased_L-2_H-128_A-2
[2023-08-09 11:37:32,586][datasets.builder][WARNING] - Reusing dataset glue (C:\Users\faizan\.cache\huggingface\datasets\glue\cola\1.0.0\dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
