[2023-08-06 18:11:49,573][__main__][INFO] - model:
  name: google/bert_uncased_L-2_H-128_A-2
  tokenizer: google/bert_uncased_L-2_H-128_A-2
preprocess:
  batch: 64
  max_length: 128
training:
  max_epochs: 1
  log_every_n_steps: 10
  deterministic: true
  limit_train_batches: 0.25
  limit_val_batches: 0.25

[2023-08-06 18:11:49,577][__main__][INFO] - Using model: google/bert_uncased_L-2_H-128_A-2
[2023-08-06 18:11:49,580][__main__][INFO] - using the tokenizer: google/bert_uncased_L-2_H-128_A-2
